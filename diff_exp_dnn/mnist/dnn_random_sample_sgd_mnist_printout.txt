##epoch:0##  loss : 0.312384/0.300978 ,   acc : 0.907250/0.911500 ,   lr : 0.100000
##epoch:0##  loss : 0.303622/0.299353 ,   acc : 0.905617/0.908200 ,   lr : 0.100000
##epoch:0##  loss : 0.210705/0.208853 ,   acc : 0.936650/0.933000 ,   lr : 0.100000
##epoch:0##  loss : 0.188841/0.187887 ,   acc : 0.945067/0.940500 ,   lr : 0.100000
##epoch:1##  loss : 0.169527/0.181250 ,   acc : 0.950617/0.944100 ,   lr : 0.100000
##epoch:1##  loss : 0.159949/0.164547 ,   acc : 0.954983/0.950700 ,   lr : 0.100000
##epoch:1##  loss : 0.211120/0.216240 ,   acc : 0.939633/0.935900 ,   lr : 0.100000
learning rate decrease to  0.01
##epoch:1##  loss : 0.148514/0.158451 ,   acc : 0.958717/0.953200 ,   lr : 0.010000
##epoch:2##  loss : 0.146669/0.157670 ,   acc : 0.959717/0.953900 ,   lr : 0.010000
##epoch:2##  loss : 0.148473/0.159945 ,   acc : 0.959800/0.953900 ,   lr : 0.010000
##epoch:2##  loss : 0.148122/0.159300 ,   acc : 0.959783/0.954100 ,   lr : 0.010000
##epoch:2##  loss : 0.148204/0.158356 ,   acc : 0.960283/0.953800 ,   lr : 0.010000
##epoch:3##  loss : 0.149048/0.159754 ,   acc : 0.959600/0.953800 ,   lr : 0.010000
learning rate decrease to  0.001
##epoch:3##  loss : 0.148601/0.159746 ,   acc : 0.960083/0.953800 ,   lr : 0.001000
##epoch:3##  loss : 0.148565/0.159833 ,   acc : 0.959967/0.954600 ,   lr : 0.001000
##epoch:3##  loss : 0.148550/0.159714 ,   acc : 0.960033/0.954200 ,   lr : 0.001000
##epoch:4##  loss : 0.148724/0.159715 ,   acc : 0.960083/0.954200 ,   lr : 0.001000
##epoch:4##  loss : 0.148570/0.159391 ,   acc : 0.960200/0.954600 ,   lr : 0.001000
##epoch:4##  loss : 0.148677/0.159645 ,   acc : 0.960083/0.954600 ,   lr : 0.001000
learning rate decrease to  0.0001
##epoch:4##  loss : 0.148695/0.159674 ,   acc : 0.960017/0.954600 ,   lr : 0.000100
##epoch:5##  loss : 0.148686/0.159667 ,   acc : 0.960050/0.954600 ,   lr : 0.000100
##epoch:5##  loss : 0.148686/0.159673 ,   acc : 0.960050/0.954400 ,   lr : 0.000100
##epoch:5##  loss : 0.148677/0.159671 ,   acc : 0.960083/0.954700 ,   lr : 0.000100
##epoch:5##  loss : 0.148671/0.159680 ,   acc : 0.960133/0.954700 ,   lr : 0.000100
##epoch:6##  loss : 0.148677/0.159680 ,   acc : 0.960150/0.954600 ,   lr : 0.000100
##epoch:6##  loss : 0.148680/0.159659 ,   acc : 0.960150/0.954600 ,   lr : 0.000100
learning rate decrease to  1e-05
##epoch:6##  loss : 0.148681/0.159663 ,   acc : 0.960150/0.954600 ,   lr : 0.000010
##epoch:6##  loss : 0.148680/0.159663 ,   acc : 0.960150/0.954600 ,   lr : 0.000010
##epoch:7##  loss : 0.148684/0.159669 ,   acc : 0.960150/0.954600 ,   lr : 0.000010
##epoch:7##  loss : 0.148685/0.159674 ,   acc : 0.960167/0.954600 ,   lr : 0.000010
##epoch:7##  loss : 0.148686/0.159674 ,   acc : 0.960167/0.954600 ,   lr : 0.000010
##epoch:7##  loss : 0.148688/0.159676 ,   acc : 0.960183/0.954600 ,   lr : 0.000010
learning rate decrease to  1.0000000000000002e-06
##epoch:8##  loss : 0.148688/0.159675 ,   acc : 0.960183/0.954600 ,   lr : 0.000001
##epoch:8##  loss : 0.148688/0.159675 ,   acc : 0.960183/0.954600 ,   lr : 0.000001
##epoch:8##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000001
##epoch:8##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000001
##epoch:9##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000001
##epoch:9##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000001
##epoch:9##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000001
##epoch:9##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000001
learning rate decrease to  1.0000000000000002e-07
##epoch:10##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:10##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:10##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:10##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:11##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:11##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:11##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-08
##epoch:11##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:12##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:12##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:12##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:12##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:13##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-09
##epoch:13##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:13##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:13##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:14##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:14##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:14##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-10
##epoch:14##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:15##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:15##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:15##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:15##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:16##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-11
##epoch:16##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:16##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:16##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:17##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:17##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:17##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-12
##epoch:17##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:18##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:18##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:18##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:18##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:19##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-13
##epoch:19##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:19##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:19##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:20##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:20##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:20##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-14
##epoch:20##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:21##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:21##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:21##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:21##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:22##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1e-15
##epoch:22##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:22##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:22##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:23##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:23##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:23##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-16
##epoch:23##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:24##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:24##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:24##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:24##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:25##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1e-17
##epoch:25##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:25##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:25##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:26##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:26##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:26##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1e-18
##epoch:26##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:27##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:27##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:27##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:27##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:28##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-19
##epoch:28##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:28##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:28##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:29##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:29##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:29##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-20
##epoch:29##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:30##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:30##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:30##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:30##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:31##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-21
##epoch:31##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:31##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:31##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:32##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:32##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:32##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1e-22
##epoch:32##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:33##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:33##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:33##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:33##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:34##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-23
##epoch:34##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:34##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:34##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:35##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:35##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:35##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-24
##epoch:35##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:36##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:36##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:36##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:36##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:37##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-25
##epoch:37##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:37##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:37##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:38##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:38##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:38##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-26
##epoch:38##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:39##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:39##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:39##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:39##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:40##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-27
##epoch:40##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:40##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:40##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:41##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:41##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:41##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-28
##epoch:41##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:42##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:42##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:42##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:42##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:43##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-29
##epoch:43##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:43##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:43##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:44##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:44##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:44##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-30
##epoch:44##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:45##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:45##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:45##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:45##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:46##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-31
##epoch:46##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:46##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:46##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:47##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:47##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:47##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-32
##epoch:47##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:48##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:48##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:48##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:48##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:49##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-33
##epoch:49##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:49##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:49##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:50##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:50##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:50##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-34
##epoch:50##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:51##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:51##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:51##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:51##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:52##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-35
##epoch:52##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:52##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:52##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:53##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:53##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:53##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-36
##epoch:53##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:54##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:54##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:54##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:54##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:55##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000005e-37
##epoch:55##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:55##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:55##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:56##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:56##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:56##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000005e-38
##epoch:56##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:57##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:57##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:57##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:57##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:58##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-39
##epoch:58##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:58##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:58##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:59##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:59##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:59##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-40
##epoch:59##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:60##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:60##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:60##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:60##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:61##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-41
##epoch:61##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:61##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:61##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:62##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:62##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:62##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-42
##epoch:62##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:63##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:63##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:63##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:63##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:64##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-43
##epoch:64##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:64##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:64##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:65##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:65##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:65##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-44
##epoch:65##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:66##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:66##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:66##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:66##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:67##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-45
##epoch:67##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:67##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:67##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:68##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:68##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:68##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-46
##epoch:68##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:69##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:69##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:69##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:69##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:70##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-47
##epoch:70##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:70##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:70##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:71##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:71##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:71##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-48
##epoch:71##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:72##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:72##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:72##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:72##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:73##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-49
##epoch:73##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:73##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:73##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:74##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:74##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:74##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-50
##epoch:74##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:75##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:75##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:75##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:75##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:76##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-51
##epoch:76##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:76##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:76##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:77##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:77##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:77##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-52
##epoch:77##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:78##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:78##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:78##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:78##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:79##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-53
##epoch:79##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:79##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:79##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:80##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:80##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:80##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-54
##epoch:80##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:81##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:81##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:81##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:81##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:82##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-55
##epoch:82##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:82##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:82##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:83##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:83##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:83##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-56
##epoch:83##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:84##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:84##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:84##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:84##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:85##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-57
##epoch:85##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:85##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:85##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:86##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:86##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:86##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-58
##epoch:86##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:87##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:87##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000
##epoch:87##  loss : 0.148688/0.159675 ,   acc : 0.960167/0.954600 ,   lr : 0.000000