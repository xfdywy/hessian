##epoch:0##  loss : 2.182407/2.182394 ,   acc : 0.189140/0.191500 ,   lr : 0.100000
##epoch:0##  loss : 1.890241/1.892306 ,   acc : 0.319540/0.318500 ,   lr : 0.100000
##epoch:0##  loss : 1.899673/1.897271 ,   acc : 0.311140/0.305600 ,   lr : 0.100000
##epoch:0##  loss : 1.802353/1.796058 ,   acc : 0.352740/0.346700 ,   lr : 0.100000
##epoch:1##  loss : 1.737051/1.735860 ,   acc : 0.371460/0.372800 ,   lr : 0.100000
##epoch:1##  loss : 1.764974/1.765297 ,   acc : 0.367780/0.370000 ,   lr : 0.100000
##epoch:1##  loss : 1.752151/1.763212 ,   acc : 0.369840/0.369300 ,   lr : 0.100000
##epoch:1##  loss : 1.787594/1.790182 ,   acc : 0.364620/0.359800 ,   lr : 0.100000
learning rate decrease to  0.01
##epoch:2##  loss : 1.610956/1.625309 ,   acc : 0.428400/0.426100 ,   lr : 0.010000
##epoch:2##  loss : 1.604328/1.620999 ,   acc : 0.434520/0.427100 ,   lr : 0.010000
##epoch:2##  loss : 1.603367/1.621654 ,   acc : 0.433360/0.424600 ,   lr : 0.010000
##epoch:2##  loss : 1.591668/1.609312 ,   acc : 0.438720/0.432500 ,   lr : 0.010000
##epoch:3##  loss : 1.582783/1.601405 ,   acc : 0.443380/0.434500 ,   lr : 0.010000
##epoch:3##  loss : 1.576319/1.594809 ,   acc : 0.444340/0.437900 ,   lr : 0.010000
##epoch:3##  loss : 1.585613/1.608626 ,   acc : 0.442700/0.434600 ,   lr : 0.010000
##epoch:3##  loss : 1.579257/1.604438 ,   acc : 0.445680/0.437900 ,   lr : 0.010000
##epoch:4##  loss : 1.571445/1.599172 ,   acc : 0.449940/0.438200 ,   lr : 0.010000
##epoch:4##  loss : 1.558026/1.586007 ,   acc : 0.452300/0.445500 ,   lr : 0.010000
##epoch:4##  loss : 1.561713/1.590770 ,   acc : 0.453120/0.446600 ,   lr : 0.010000
##epoch:4##  loss : 1.558882/1.588614 ,   acc : 0.452880/0.442900 ,   lr : 0.010000
##epoch:5##  loss : 1.551471/1.583869 ,   acc : 0.455360/0.444000 ,   lr : 0.010000
##epoch:5##  loss : 1.546322/1.579327 ,   acc : 0.456680/0.447300 ,   lr : 0.010000
##epoch:5##  loss : 1.562014/1.597518 ,   acc : 0.455000/0.443200 ,   lr : 0.010000
learning rate decrease to  0.001
##epoch:5##  loss : 1.534841/1.571337 ,   acc : 0.464480/0.453500 ,   lr : 0.001000
##epoch:6##  loss : 1.533608/1.571685 ,   acc : 0.464160/0.449200 ,   lr : 0.001000
##epoch:6##  loss : 1.532168/1.569932 ,   acc : 0.464700/0.451100 ,   lr : 0.001000
##epoch:6##  loss : 1.531883/1.569466 ,   acc : 0.465000/0.451000 ,   lr : 0.001000
##epoch:6##  loss : 1.531437/1.569412 ,   acc : 0.465300/0.450800 ,   lr : 0.001000
##epoch:7##  loss : 1.529771/1.568661 ,   acc : 0.466140/0.451600 ,   lr : 0.001000
##epoch:7##  loss : 1.529192/1.568683 ,   acc : 0.466220/0.452500 ,   lr : 0.001000
##epoch:7##  loss : 1.530478/1.570307 ,   acc : 0.465520/0.450500 ,   lr : 0.001000
##epoch:7##  loss : 1.531735/1.570966 ,   acc : 0.466000/0.451800 ,   lr : 0.001000
learning rate decrease to  0.0001
##epoch:8##  loss : 1.529762/1.568970 ,   acc : 0.466360/0.452000 ,   lr : 0.000100
##epoch:8##  loss : 1.529450/1.568903 ,   acc : 0.466700/0.451900 ,   lr : 0.000100
##epoch:8##  loss : 1.529367/1.569145 ,   acc : 0.467340/0.453200 ,   lr : 0.000100
##epoch:8##  loss : 1.529115/1.569026 ,   acc : 0.467360/0.452500 ,   lr : 0.000100
##epoch:9##  loss : 1.529258/1.569274 ,   acc : 0.467420/0.453100 ,   lr : 0.000100
##epoch:9##  loss : 1.529165/1.569209 ,   acc : 0.466940/0.452500 ,   lr : 0.000100
##epoch:9##  loss : 1.528939/1.569084 ,   acc : 0.467280/0.453100 ,   lr : 0.000100
##epoch:9##  loss : 1.528570/1.568685 ,   acc : 0.467580/0.452500 ,   lr : 0.000100
##epoch:10##  loss : 1.528752/1.569136 ,   acc : 0.467180/0.451900 ,   lr : 0.000100
##epoch:10##  loss : 1.528489/1.568968 ,   acc : 0.467580/0.451600 ,   lr : 0.000100
##epoch:10##  loss : 1.528381/1.568763 ,   acc : 0.467400/0.452300 ,   lr : 0.000100
##epoch:10##  loss : 1.528120/1.568583 ,   acc : 0.467260/0.451900 ,   lr : 0.000100
##epoch:11##  loss : 1.528111/1.568753 ,   acc : 0.467420/0.452600 ,   lr : 0.000100
##epoch:11##  loss : 1.528230/1.568982 ,   acc : 0.467220/0.452400 ,   lr : 0.000100
##epoch:11##  loss : 1.528139/1.568802 ,   acc : 0.467320/0.452000 ,   lr : 0.000100
##epoch:11##  loss : 1.528080/1.568831 ,   acc : 0.467200/0.452700 ,   lr : 0.000100
##epoch:12##  loss : 1.528128/1.568948 ,   acc : 0.466980/0.452300 ,   lr : 0.000100
##epoch:12##  loss : 1.528011/1.568857 ,   acc : 0.467280/0.452400 ,   lr : 0.000100
##epoch:12##  loss : 1.527766/1.568524 ,   acc : 0.467300/0.452700 ,   lr : 0.000100
##epoch:12##  loss : 1.527871/1.568701 ,   acc : 0.467340/0.452600 ,   lr : 0.000100
##epoch:13##  loss : 1.528434/1.569265 ,   acc : 0.467700/0.453700 ,   lr : 0.000100
learning rate decrease to  1e-05
##epoch:13##  loss : 1.528303/1.569115 ,   acc : 0.467840/0.453900 ,   lr : 0.000010
##epoch:13##  loss : 1.528211/1.569032 ,   acc : 0.467800/0.454200 ,   lr : 0.000010
##epoch:13##  loss : 1.528128/1.568943 ,   acc : 0.467860/0.453800 ,   lr : 0.000010
##epoch:14##  loss : 1.528014/1.568836 ,   acc : 0.467580/0.453600 ,   lr : 0.000010
##epoch:14##  loss : 1.527987/1.568815 ,   acc : 0.467680/0.453200 ,   lr : 0.000010
##epoch:14##  loss : 1.527949/1.568776 ,   acc : 0.467720/0.453100 ,   lr : 0.000010
##epoch:14##  loss : 1.527915/1.568740 ,   acc : 0.467780/0.453400 ,   lr : 0.000010
##epoch:15##  loss : 1.527878/1.568735 ,   acc : 0.467620/0.452900 ,   lr : 0.000010
##epoch:15##  loss : 1.527891/1.568763 ,   acc : 0.467620/0.452800 ,   lr : 0.000010
##epoch:15##  loss : 1.527847/1.568714 ,   acc : 0.467460/0.452900 ,   lr : 0.000010
##epoch:15##  loss : 1.527880/1.568768 ,   acc : 0.467540/0.453000 ,   lr : 0.000010
##epoch:16##  loss : 1.527797/1.568671 ,   acc : 0.467680/0.453100 ,   lr : 0.000010
##epoch:16##  loss : 1.527784/1.568663 ,   acc : 0.467740/0.453100 ,   lr : 0.000010
##epoch:16##  loss : 1.527732/1.568603 ,   acc : 0.467780/0.453100 ,   lr : 0.000010
##epoch:16##  loss : 1.527691/1.568547 ,   acc : 0.467660/0.452800 ,   lr : 0.000010
##epoch:17##  loss : 1.527665/1.568519 ,   acc : 0.467780/0.452900 ,   lr : 0.000010
##epoch:17##  loss : 1.527703/1.568563 ,   acc : 0.467820/0.453100 ,   lr : 0.000010
##epoch:17##  loss : 1.527721/1.568602 ,   acc : 0.467760/0.453000 ,   lr : 0.000010
learning rate decrease to  1.0000000000000002e-06
##epoch:17##  loss : 1.527723/1.568605 ,   acc : 0.467720/0.453000 ,   lr : 0.000001
##epoch:18##  loss : 1.527723/1.568607 ,   acc : 0.467800/0.453000 ,   lr : 0.000001
##epoch:18##  loss : 1.527721/1.568605 ,   acc : 0.467800/0.453000 ,   lr : 0.000001
##epoch:18##  loss : 1.527721/1.568605 ,   acc : 0.467800/0.453000 ,   lr : 0.000001
##epoch:18##  loss : 1.527717/1.568603 ,   acc : 0.467840/0.452900 ,   lr : 0.000001
##epoch:19##  loss : 1.527718/1.568605 ,   acc : 0.467800/0.452800 ,   lr : 0.000001
##epoch:19##  loss : 1.527717/1.568606 ,   acc : 0.467800/0.453000 ,   lr : 0.000001
##epoch:19##  loss : 1.527715/1.568602 ,   acc : 0.467780/0.453000 ,   lr : 0.000001
##epoch:19##  loss : 1.527708/1.568597 ,   acc : 0.467780/0.452900 ,   lr : 0.000001
##epoch:20##  loss : 1.527704/1.568594 ,   acc : 0.467760/0.453100 ,   lr : 0.000001
##epoch:20##  loss : 1.527699/1.568589 ,   acc : 0.467840/0.452900 ,   lr : 0.000001
##epoch:20##  loss : 1.527693/1.568583 ,   acc : 0.467840/0.452900 ,   lr : 0.000001
##epoch:20##  loss : 1.527692/1.568583 ,   acc : 0.467840/0.453000 ,   lr : 0.000001
##epoch:21##  loss : 1.527692/1.568588 ,   acc : 0.467840/0.452900 ,   lr : 0.000001
##epoch:21##  loss : 1.527692/1.568590 ,   acc : 0.467840/0.452800 ,   lr : 0.000001
##epoch:21##  loss : 1.527686/1.568582 ,   acc : 0.467860/0.452800 ,   lr : 0.000001
##epoch:21##  loss : 1.527687/1.568583 ,   acc : 0.467840/0.453000 ,   lr : 0.000001
##epoch:22##  loss : 1.527686/1.568583 ,   acc : 0.467840/0.452900 ,   lr : 0.000001
##epoch:22##  loss : 1.527686/1.568583 ,   acc : 0.467840/0.452800 ,   lr : 0.000001
##epoch:22##  loss : 1.527684/1.568582 ,   acc : 0.467800/0.452800 ,   lr : 0.000001
##epoch:22##  loss : 1.527679/1.568576 ,   acc : 0.467800/0.452800 ,   lr : 0.000001
##epoch:23##  loss : 1.527678/1.568574 ,   acc : 0.467780/0.453000 ,   lr : 0.000001
##epoch:23##  loss : 1.527678/1.568574 ,   acc : 0.467820/0.453000 ,   lr : 0.000001
##epoch:23##  loss : 1.527679/1.568577 ,   acc : 0.467800/0.452800 ,   lr : 0.000001
##epoch:23##  loss : 1.527673/1.568569 ,   acc : 0.467800/0.452900 ,   lr : 0.000001
##epoch:24##  loss : 1.527672/1.568570 ,   acc : 0.467820/0.453000 ,   lr : 0.000001
##epoch:24##  loss : 1.527672/1.568571 ,   acc : 0.467820/0.453000 ,   lr : 0.000001
##epoch:24##  loss : 1.527671/1.568572 ,   acc : 0.467840/0.453000 ,   lr : 0.000001
##epoch:24##  loss : 1.527669/1.568570 ,   acc : 0.467800/0.453000 ,   lr : 0.000001
##epoch:25##  loss : 1.527675/1.568576 ,   acc : 0.467880/0.453000 ,   lr : 0.000001
learning rate decrease to  1.0000000000000002e-07
##epoch:25##  loss : 1.527674/1.568576 ,   acc : 0.467880/0.453000 ,   lr : 0.000000
##epoch:25##  loss : 1.527674/1.568576 ,   acc : 0.467880/0.453000 ,   lr : 0.000000
##epoch:25##  loss : 1.527674/1.568575 ,   acc : 0.467880/0.453000 ,   lr : 0.000000
##epoch:26##  loss : 1.527673/1.568575 ,   acc : 0.467880/0.453000 ,   lr : 0.000000
##epoch:26##  loss : 1.527674/1.568575 ,   acc : 0.467880/0.453000 ,   lr : 0.000000
##epoch:26##  loss : 1.527673/1.568575 ,   acc : 0.467880/0.453000 ,   lr : 0.000000
##epoch:26##  loss : 1.527673/1.568575 ,   acc : 0.467880/0.453000 ,   lr : 0.000000
##epoch:27##  loss : 1.527673/1.568574 ,   acc : 0.467880/0.453000 ,   lr : 0.000000
##epoch:27##  loss : 1.527673/1.568574 ,   acc : 0.467880/0.453000 ,   lr : 0.000000
##epoch:27##  loss : 1.527673/1.568574 ,   acc : 0.467880/0.452900 ,   lr : 0.000000
##epoch:27##  loss : 1.527672/1.568574 ,   acc : 0.467880/0.452900 ,   lr : 0.000000
##epoch:28##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-08
##epoch:28##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:28##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:28##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:29##  loss : 1.527672/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:29##  loss : 1.527672/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:29##  loss : 1.527672/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:29##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-09
##epoch:30##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:30##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:30##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:30##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:31##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:31##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-10
##epoch:31##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:31##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:32##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:32##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:32##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:32##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-11
##epoch:33##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:33##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:33##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:33##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:34##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:34##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-12
##epoch:34##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:34##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:35##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:35##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:35##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:35##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-13
##epoch:36##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:36##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:36##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:36##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:37##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:37##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-14
##epoch:37##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:37##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:38##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:38##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:38##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:38##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1e-15
##epoch:39##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:39##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:39##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:39##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:40##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:40##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-16
##epoch:40##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:40##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:41##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:41##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:41##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:41##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1e-17
##epoch:42##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:42##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:42##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:42##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:43##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:43##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1e-18
##epoch:43##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:43##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:44##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:44##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:44##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:44##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-19
##epoch:45##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:45##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:45##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:45##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:46##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:46##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-20
##epoch:46##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:46##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:47##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:47##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:47##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:47##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-21
##epoch:48##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:48##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:48##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:48##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:49##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:49##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1e-22
##epoch:49##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:49##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:50##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:50##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:50##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:50##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-23
##epoch:51##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:51##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:51##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:51##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:52##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:52##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000001e-24
##epoch:52##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:52##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:53##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:53##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:53##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:53##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-25
##epoch:54##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:54##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:54##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:54##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:55##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:55##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-26
##epoch:55##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:55##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:56##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:56##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:56##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:56##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-27
##epoch:57##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:57##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:57##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:57##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:58##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:58##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-28
##epoch:58##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:58##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:59##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:59##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:59##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:59##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-29
##epoch:60##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:60##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:60##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:60##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:61##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:61##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-30
##epoch:61##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:61##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:62##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:62##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:62##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:62##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-31
##epoch:63##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:63##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:63##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:63##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:64##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:64##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-32
##epoch:64##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:64##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:65##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:65##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:65##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:65##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-33
##epoch:66##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:66##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:66##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:66##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:67##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:67##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-34
##epoch:67##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:67##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:68##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:68##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:68##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:68##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-35
##epoch:69##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:69##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:69##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:69##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:70##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:70##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-36
##epoch:70##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:70##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:71##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:71##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:71##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:71##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000005e-37
##epoch:72##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:72##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:72##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:72##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:73##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:73##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000005e-38
##epoch:73##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:73##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:74##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:74##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:74##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:74##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-39
##epoch:75##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:75##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:75##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:75##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:76##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:76##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-40
##epoch:76##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:76##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:77##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:77##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:77##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:77##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-41
##epoch:78##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:78##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:78##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:78##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:79##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:79##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000004e-42
##epoch:79##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:79##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:80##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:80##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:80##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:80##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-43
##epoch:81##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:81##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:81##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:81##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:82##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:82##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-44
##epoch:82##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:82##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:83##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:83##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:83##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:83##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-45
##epoch:84##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:84##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:84##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:84##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:85##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:85##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-46
##epoch:85##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:85##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:86##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:86##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:86##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:86##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000002e-47
##epoch:87##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:87##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:87##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:87##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:88##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:88##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
learning rate decrease to  1.0000000000000003e-48
##epoch:88##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:88##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000
##epoch:89##  loss : 1.527673/1.568574 ,   acc : 0.467900/0.452900 ,   lr : 0.000000